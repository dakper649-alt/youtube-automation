"""
Script Generator - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è YouTube —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: Hugging Face (Llama/Qwen/Mistral), Groq (Llama 3.1 70B), Google Gemini 2.0 Flash, OpenAI GPT-4o-mini
"""

import os
from typing import Dict, List, Optional
import time
import httpx

# –ù–æ–≤–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ Gemini
from google import genai
from google.genai import types

# OpenAI –¥–ª—è fallback
from openai import OpenAI

class ScriptGeneratorError(Exception):
    """–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∫—Ä–∏–ø—Ç–∞"""
    pass

class ScriptGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä YouTube —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤"""

    def __init__(self, api_key_manager, provider: str = 'gemini'):
        """
        Args:
            api_key_manager: SafeAPIManager instance
            provider: 'gemini' –∏–ª–∏ 'openai'
        """
        self.key_manager = api_key_manager
        self.provider = provider.lower()

        # Gemini setup (–Ω–æ–≤–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞)
        if self.provider == 'gemini':
            gemini_key = self.key_manager.get_gemini_key()
            if not gemini_key:
                raise ScriptGeneratorError("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö Gemini API –∫–ª—é—á–µ–π")

            self.client = genai.Client(api_key=gemini_key)
            self.model_id = 'gemini-2.0-flash-exp'
            print(f"‚úÖ ScriptGenerator: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Gemini 2.0 Flash")

        # OpenAI setup (fallback)
        elif self.provider == 'openai':
            openai_key = os.getenv('OPENAI_API_KEY')
            if not openai_key:
                raise ScriptGeneratorError("–ù–µ—Ç OPENAI_API_KEY –≤ .env")

            self.client = OpenAI(api_key=openai_key)
            print(f"‚úÖ ScriptGenerator: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenAI GPT-4o-mini")

        else:
            raise ScriptGeneratorError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π provider: {provider}")

    async def _generate_with_ollama(self, prompt: str) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é Ollama –º–æ–¥–µ–ª—å

        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
        1. Ollama –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: brew install ollama
        2. –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: ollama pull llama3.1:8b
        3. –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω: ollama serve (–≤ —Ñ–æ–Ω–µ)
        """

        print("   üñ•Ô∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é Ollama (Llama 3.1 8B)...")

        try:
            # Ollama API endpoint (–ª–æ–∫–∞–ª—å–Ω—ã–π)
            url = "http://localhost:11434/api/generate"

            payload = {
                "model": "llama3.1:8b",
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 4000,  # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
                }
            }

            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(url, json=payload)

                if response.status_code == 200:
                    result = response.json()
                    generated_text = result.get('response', '')

                    if generated_text:
                        print(f"   ‚úÖ Ollama —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ {len(generated_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                        return generated_text
                    else:
                        raise Exception("Ollama –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                else:
                    raise Exception(f"Ollama API error: {response.status_code}")

        except httpx.ConnectError:
            raise Exception(
                "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama!\n"
                "–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ:\n"
                "1. Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: brew install ollama\n"
                "2. –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω: ollama serve\n"
                "3. –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: ollama pull llama3.1:8b"
            )
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ Ollama: {str(e)}")

    async def generate_script(
        self,
        topic: str,
        target_length: int = 1000,
        language: str = 'ru',
        niche: str = 'psychology',
        use_ollama: bool = True
    ) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ü–µ–Ω–∞—Ä–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –Ω–∞ Ollama

        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤:
        1. Ollama (–ª–æ–∫–∞–ª—å–Ω–æ, –±–µ—Å–ø–ª–∞—Ç–Ω–æ) - –µ—Å–ª–∏ use_ollama=True
        2. Hugging Face (123 –∫–ª—é—á–∞)
        3. Groq (14,400 –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å)
        4. Gemini 2.0 Flash
        5. OpenAI GPT-4o-mini

        Args:
            topic: –¢–µ–º–∞ –≤–∏–¥–µ–æ
            target_length: –¶–µ–ª–µ–≤–∞—è –¥–ª–∏–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ö
            language: –Ø–∑—ã–∫ ('ru' –∏–ª–∏ 'en')
            niche: –ù–∏—à–∞
            use_ollama: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é Ollama (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)

        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏: hook, script, cta, title_suggestions, word_count, provider, model
        """

        prompt = self._build_prompt(topic, target_length, language, niche)

        # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –ø–æ –ø–æ—Ä—è–¥–∫—É: Ollama -> HF -> Groq -> Gemini -> OpenAI
        errors = []

        # 0. –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ Ollama (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
        if use_ollama:
            try:
                print("üñ•Ô∏è –ü—Ä–æ–±—É–µ–º Ollama (–ª–æ–∫–∞–ª—å–Ω–∞—è Llama 3.1 8B)...")
                content = await self._generate_with_ollama(prompt)
                result = self._parse_response(content)
                result['word_count'] = len(result['script'].split())
                result['provider'] = 'ollama'
                result['model'] = 'llama3.1:8b'
                print("‚úÖ –°–∫—Ä–∏–ø—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —á–µ—Ä–µ–∑ Ollama (–ë–ï–°–ü–õ–ê–¢–ù–û!)")
                return result
            except Exception as e:
                print(f"‚ö†Ô∏è  Ollama –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
                print(f"   üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –æ–±–ª–∞—á–Ω—ã–µ API...")
                errors.append(f"Ollama: {e}")

        # 1. –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ Hugging Face (–ë–ï–°–ü–õ–ê–¢–ù–û! 125 –∫–ª—é—á–µ–π!)
        try:
            print("ü§ó –ü—Ä–æ–±—É–µ–º Hugging Face API (Llama 3.1 / Qwen / Mistral)...")
            content = await self._generate_with_huggingface(prompt)
            result = self._parse_response(content)
            result['word_count'] = len(result['script'].split())
            result['provider'] = 'huggingface'
            result['model'] = 'llama-3.1-8b'
            print("‚úÖ –°–∫—Ä–∏–ø—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —á–µ—Ä–µ–∑ Hugging Face")
            return result
        except Exception as e:
            print(f"‚ö†Ô∏è  Hugging Face failed: {e}")
            errors.append(f"Hugging Face: {e}")

        # 2. –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ Groq (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
        try:
            print("üöÄ –ü—Ä–æ–±—É–µ–º Groq API (Llama 3.1 70B)...")
            content = await self._generate_with_groq(prompt)
            result = self._parse_response(content)
            result['word_count'] = len(result['script'].split())
            result['provider'] = 'groq'
            result['model'] = 'llama-3.1-70b'
            print("‚úÖ –°–∫—Ä–∏–ø—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —á–µ—Ä–µ–∑ Groq")
            return result
        except Exception as e:
            print(f"‚ö†Ô∏è  Groq failed: {e}")
            errors.append(f"Groq: {e}")

        # 3. –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ Gemini
        try:
            print("üîÑ –ü—Ä–æ–±—É–µ–º Gemini API (Gemini 2.0 Flash)...")
            content = await self._generate_with_gemini(prompt)
            result = self._parse_response(content)
            result['word_count'] = len(result['script'].split())
            result['provider'] = 'gemini'
            result['model'] = 'gemini-2.0-flash'
            print("‚úÖ –°–∫—Ä–∏–ø—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —á–µ—Ä–µ–∑ Gemini")
            return result
        except Exception as e:
            print(f"‚ö†Ô∏è  Gemini failed: {e}")
            errors.append(f"Gemini: {e}")

        # 4. –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ OpenAI (–ø–æ—Å–ª–µ–¥–Ω–∏–π fallback)
        try:
            print("üîÑ –ü—Ä–æ–±—É–µ–º OpenAI API (GPT-4o-mini)...")
            openai_key = os.getenv('OPENAI_API_KEY')
            if not openai_key:
                raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")

            content = await self._generate_with_openai_direct(prompt, openai_key)
            result = self._parse_response(content)
            result['word_count'] = len(result['script'].split())
            result['provider'] = 'openai'
            result['model'] = 'gpt-4o-mini'
            print("‚úÖ –°–∫—Ä–∏–ø—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —á–µ—Ä–µ–∑ OpenAI")
            return result
        except Exception as e:
            print(f"‚ö†Ô∏è  OpenAI failed: {e}")
            errors.append(f"OpenAI: {e}")

        # –í—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã —É–ø–∞–ª–∏
        raise ScriptGeneratorError(f"–í—Å–µ API –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã:\n" + "\n".join(errors))

    async def _generate_with_huggingface(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Hugging Face Inference API (–ë–ï–°–ü–õ–ê–¢–ù–û!)"""

        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print(f"   üìä HF Keys —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"      –í—Å–µ–≥–æ HF –∫–ª—é—á–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(self.key_manager.hf_keys)}")

        if len(self.key_manager.hf_keys) == 0:
            raise ValueError("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö Hugging Face –∫–ª—é—á–µ–π!\n–î–æ–±–∞–≤—å—Ç–µ –≤ .env: HUGGINGFACE_API_KEY_1=hf_...")

        hf_key = self.key_manager.get_hf_key()
        print(f"      –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–ª—é—á: {hf_key[:8]}...{hf_key[-4:]}")

        # –õ—É—á—à–∏–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
        models = [
            "meta-llama/Llama-3.1-8B-Instruct",      # –ë—ã—Å—Ç—Ä–∞—è –∏ —É–º–Ω–∞—è
            "Qwen/Qwen2.5-72B-Instruct",             # –û—á–µ–Ω—å —É–º–Ω–∞—è
            "mistralai/Mistral-7B-Instruct-v0.3"     # –ù–∞–¥—ë–∂–Ω–∞—è
        ]

        print(f"      –ü—Ä–æ–±—É–µ–º {len(models)} –º–æ–¥–µ–ª–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É...")

        for model in models:
            try:
                print(f"      üîÑ –ü—Ä–æ–±—É—é –º–æ–¥–µ–ª—å: {model.split('/')[-1]}...")
                url = f"https://api-inference.huggingface.co/models/{model}"
                headers = {
                    "Authorization": f"Bearer {hf_key}",
                    "Content-Type": "application/json"
                }

                data = {
                    "inputs": prompt,
                    "parameters": {
                        "max_new_tokens": 4000,
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "return_full_text": False
                    }
                }

                async with httpx.AsyncClient(timeout=120.0) as client:
                    response = await client.post(url, headers=headers, json=data)

                    if response.status_code == 503:
                        # –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é
                        print(f"         ‚è≥ {model.split('/')[-1]} –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é...")
                        continue

                    if response.status_code != 200:
                        error_text = response.text[:200]
                        print(f"         ‚ùå HTTP {response.status_code}: {error_text}")
                        continue

                    result = response.json()
                    print(f"         ‚úÖ {model.split('/')[-1]} –æ—Ç–≤–µ—Ç–∏–ª–∞ —É—Å–ø–µ—à–Ω–æ!")

                    # Hugging Face –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Å—Å–∏–≤ –∏–ª–∏ –æ–±—ä–µ–∫—Ç
                    if isinstance(result, list) and len(result) > 0:
                        return result[0].get('generated_text', '')
                    elif isinstance(result, dict):
                        return result.get('generated_text', '')

            except Exception as e:
                print(f"         ‚ö†Ô∏è  {model.split('/')[-1]} failed: {str(e)[:100]}")
                continue

        raise Exception("–í—Å–µ Hugging Face –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")

    async def _generate_with_groq(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Groq API (Llama 3.1 70B)"""

        groq_key = self.key_manager.get_groq_key()
        if not groq_key:
            raise ValueError("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö Groq API –∫–ª—é—á–µ–π!")

        url = "https://api.groq.com/openai/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {groq_key}",
            "Content-Type": "application/json"
        }

        data = {
            "model": "llama-3.1-70b-versatile",
            "messages": [
                {"role": "system", "content": "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π YouTube —Å—Ü–µ–Ω–∞—Ä–∏—Å—Ç."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 4000
        }

        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(url, headers=headers, json=data)
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content']

    async def _generate_with_gemini(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π Gemini API"""

        response = self.client.models.generate_content(
            model=self.model_id,
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0.9,
                max_output_tokens=3000,
                response_modalities=['TEXT']
            )
        )

        return response.text

    async def _generate_with_openai(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI"""

        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π YouTube —Å—Ü–µ–Ω–∞—Ä–∏—Å—Ç."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.9,
            max_tokens=3000
        )

        return response.choices[0].message.content

    async def _generate_with_openai_direct(self, prompt: str, api_key: str) -> str:
        """–ü—Ä—è–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI (–¥–ª—è fallback)"""

        client = OpenAI(api_key=api_key)

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π YouTube —Å—Ü–µ–Ω–∞—Ä–∏—Å—Ç."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.9,
            max_tokens=3000
        )

        return response.choices[0].message.content

    def _build_prompt(
        self,
        topic: str,
        target_length: int,
        language: str,
        niche: str
    ) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è AI"""

        lang_instruction = "–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ" if language == 'ru' else "in English"

        prompt = f"""
–°–æ–∑–¥–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π YouTube —Å—Ü–µ–Ω–∞—Ä–∏–π {lang_instruction} –¥–ª—è –≤–∏–¥–µ–æ –Ω–∞ —Ç–µ–º—É:
"{topic}"

–ù–ò–®–ê: {niche}
–¶–ï–õ–ï–í–ê–Ø –î–õ–ò–ù–ê: ~{target_length} —Å–ª–æ–≤

–°–¢–†–£–ö–¢–£–†–ê –°–¶–ï–ù–ê–†–ò–Ø:

[HOOK]
(–ù–∞–ø–∏—à–∏ —Ü–µ–ø–ª—è—é—â–∏–π hook –Ω–∞ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∑–∞—Å—Ç–∞–≤–∏—Ç –∑—Ä–∏—Ç–µ–ª—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å–º–æ—Ç—Ä–µ—Ç—å)

[SCRIPT]
(–ù–∞–ø–∏—à–∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ü–µ–Ω–∞—Ä–∏–π –¥–ª–∏–Ω–æ–π ~{target_length} —Å–ª–æ–≤.
–ò—Å–ø–æ–ª—å–∑—É–π:
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
- –ò—Å—Ç–æ—Ä–∏–∏
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã
- –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã
–†–∞–∑–¥–µ–ª–∏ –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏ —Å –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏)

[CTA]
(–ù–∞–ø–∏—à–∏ –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é –Ω–∞ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: –ø–æ–¥–ø–∏—Å–∫–∞, –ª–∞–π–∫, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏)

[TITLES]
(–ü—Ä–µ–¥–ª–æ–∂–∏ 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è –≤–∏–¥–µ–æ)

–í–ê–ñ–ù–û:
- –ü–∏—à–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–º —è–∑—ã–∫–æ–º
- –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
- –ò–∑–±–µ–≥–∞–π —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
- –î–µ–ª–∞–π –ø–∞—É–∑—ã –¥–ª—è –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤
"""

        return prompt.strip()

    def _parse_response(self, content: str) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ AI"""

        result = {
            'hook': '',
            'script': '',
            'cta': '',
            'title_suggestions': []
        }

        # –ü–∞—Ä—Å–∏–Ω–≥ —Å–µ–∫—Ü–∏–π
        sections = {
            'hook': ('[HOOK]', '[SCRIPT]'),
            'script': ('[SCRIPT]', '[CTA]'),
            'cta': ('[CTA]', '[TITLES]'),
            'titles': ('[TITLES]', None)
        }

        for key, (start_marker, end_marker) in sections.items():
            try:
                start_idx = content.find(start_marker)
                if start_idx == -1:
                    continue

                start_idx += len(start_marker)

                if end_marker:
                    end_idx = content.find(end_marker, start_idx)
                    if end_idx == -1:
                        section_text = content[start_idx:]
                    else:
                        section_text = content[start_idx:end_idx]
                else:
                    section_text = content[start_idx:]

                section_text = section_text.strip()

                if key == 'titles':
                    # –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                    lines = [line.strip() for line in section_text.split('\n') if line.strip()]
                    titles = []
                    for line in lines:
                        # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä–∞ –∏ –º–∞—Ä–∫–µ—Ä—ã
                        clean_line = line.lstrip('0123456789.-) ').strip()
                        if clean_line:
                            titles.append(clean_line)
                    result['title_suggestions'] = titles[:3]
                else:
                    result[key] = section_text

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–µ–∫—Ü–∏–∏ {key}: {e}")

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not result['script']:
            raise ScriptGeneratorError("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ü–µ–Ω–∞—Ä–∏–π")

        return result
